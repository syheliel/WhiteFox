{
    "summary": "\nThe `graphsafe_rng_functionalization` flag controls whether CUDA graph operations should use functionalized random number generation (RNG). When enabled:\n1. It converts torch RNG operations to their functional Philox RNG equivalents\n2. Currently only functionalizes CUDA RNG operations\n3. May affect numerical precision in CUDA graph operations\n4. Being True by default means precision changes are silently applied\n5. Disabling could lead to different RNG behavior in CUDA graphs\n",
    "python_code": "\nimport torch\n\n# Create a simple model that uses dropout (which uses RNG)\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(10, 10),\n    torch.nn.Dropout(p=0.5)\n).cuda()\n\n# This will use the graphsafe_rng_functionalization setting\n# when running under CUDA graph capture\ng = torch.cuda.CUDAGraph()\nwith torch.cuda.graph(g):\n    x = torch.randn(10, 10, device='cuda')\n    output = model(x)\n",
    "api": [
        "torch.nn.Dropout",
        "torch.nn.functional.dropout",
        "torch.rand",
        "torch.randn",
        "torch.randint"
    ]
}