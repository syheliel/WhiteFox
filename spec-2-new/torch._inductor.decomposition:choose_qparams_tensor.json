{
    "summary": "\nThe choose_qparams_tensor function calculates quantization parameters (scale and zero point) for a given input tensor. The vulnerable line computes the scale factor by dividing the input range by the quantization range. This is critical because:\n1. Precision loss can occur during the floating-point division\n2. The scale factor directly affects quantization accuracy\n3. Improper scaling can lead to significant quantization errors\n4. The calculation must maintain numerical stability for proper tensor quantization\n",
    "python_code": "\nimport torch\nimport torch.nn as nn\n\nclass QuantizedModel(nn.Module):\n    def __init__(self):\n        super(QuantizedModel, self).__init__()\n        self.linear = nn.Linear(10, 5)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # Simulate quantization parameter calculation\n        min_val, max_val = torch.aminmax(x)\n        quant_min, quant_max = 0, 255\n        scale = (max_val - min_val) / float(quant_max - quant_min)\n        return scale\n",
    "api": [
        "nn.quantized.Linear",
        "nn.quantized.Conv2d",
        "torch.quantize_per_tensor",
        "torch.quantize_per_channel",
        "torch.fake_quantize_per_tensor_affine"
    ]
}