{
    "summary": "\nThe choose_qparams_tensor function calculates quantization parameters (scale and zero point) for a given input tensor. The vulnerable line computes the scale factor by dividing the input range by the quantization range. This is important because:\n1. Precision loss can occur during the floating-point division\n2. The scale factor directly affects quantization accuracy\n3. Improper scaling can lead to significant quantization errors\n4. The calculation assumes symmetric quantization ranges\n```\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass QuantizedModel(nn.Module):\n    def __init__(self):\n        super(QuantizedModel, self).__init__()\n        self.linear = nn.Linear(10, 5)\n\n    def forward(self, x):\n        # Simulate quantization parameter calculation\n        x = self.linear(x)\n        min_val, max_val = torch.aminmax(x)\n        quant_min, quant_max = 0, 255\n        scale = (max_val - min_val) / float(quant_max - quant_min)\n        return scale * x\n```\n\n```yaml\n- nn.quantized.Linear\n- nn.quantized.Conv2d\n- torch.quantize_per_tensor\n- torch.quantize_per_channel\n- torch.fake_quantize_per_tensor_affine\n",
    "python_code": "\nimport torch\nimport torch.nn as nn\n\nclass QuantizedModel(nn.Module):\n    def __init__(self):\n        super(QuantizedModel, self).__init__()\n        self.linear = nn.Linear(10, 5)\n\n    def forward(self, x):\n        # Simulate quantization parameter calculation\n        x = self.linear(x)\n        min_val, max_val = torch.aminmax(x)\n        quant_min, quant_max = 0, 255\n        scale = (max_val - min_val) / float(quant_max - quant_min)\n        return scale * x\n```\n\n```yaml\n- nn.quantized.Linear\n- nn.quantized.Conv2d\n- torch.quantize_per_tensor\n- torch.quantize_per_channel\n- torch.fake_quantize_per_tensor_affine\n",
    "api": [
        "nn.quantized.Linear",
        "nn.quantized.Conv2d",
        "torch.quantize_per_tensor",
        "torch.quantize_per_channel",
        "torch.fake_quantize_per_tensor_affine"
    ]
}