{
    "summary": "\nThe mul_softmax_pattern function handles a common pattern in softmax computations where input scaling is applied before the softmax operation. The vulnerable line performs a numerically unstable computation that:\n1. Scales the input by a sign factor\n2. Computes the maximum value along a dimension\n3. Subtracts this maximum and rescales\n4. This pattern can lead to numerical instability due to potential loss of precision in the scaling operations\n5. The instability arises from separate scaling operations before and after max subtraction\n```\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass ScaledSoftmax(nn.Module):\n    def __init__(self, dim=-1):\n        super().__init__()\n        self.dim = dim\n        \n    def forward(self, x, scale):\n        # This demonstrates the vulnerable pattern\n        scaled = x * scale\n        max_val = scaled.amax(dim=self.dim, keepdim=True)\n        return (scaled - max_val) * scale  # Vulnerable line\n```\n\n```yaml\n- nn.Softmax\n- nn.LogSoftmax\n- torch.softmax\n- torch.log_softmax\n- torch.amax\n- torch.mul\n- torch.sub\n",
    "python_code": "\nimport torch\nimport torch.nn as nn\n\nclass ScaledSoftmax(nn.Module):\n    def __init__(self, dim=-1):\n        super().__init__()\n        self.dim = dim\n        \n    def forward(self, x, scale):\n        # This demonstrates the vulnerable pattern\n        scaled = x * scale\n        max_val = scaled.amax(dim=self.dim, keepdim=True)\n        return (scaled - max_val) * scale  # Vulnerable line\n```\n\n```yaml\n- nn.Softmax\n- nn.LogSoftmax\n- torch.softmax\n- torch.log_softmax\n- torch.amax\n- torch.mul\n- torch.sub\n",
    "api": [
        "nn.Softmax",
        "nn.LogSoftmax",
        "torch.softmax",
        "torch.log_softmax",
        "torch.amax",
        "torch.mul",
        "torch.sub"
    ]
}