{
    "summary": "\nThe triton.codegen_upcast_to_fp32 configuration controls whether float16/bfloat16 operations are automatically upcast to float32 during Triton code generation. This setting is important because:\n1. Upcasting to fp32 can improve numerical stability for some operations\n2. It may lead to unnecessary precision loss when lower precision is sufficient\n3. The conversion happens at the codegen level, affecting all Triton kernels\n4. Disabling this could improve performance by using native lower-precision operations\n",
    "python_code": "\nimport torch\nimport torch.nn as nn\n\nclass HalfPrecisionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(16, 16, dtype=torch.float16)\n        \n    def forward(self, x):\n        return self.linear(x)\n\nmodel = HalfPrecisionModel()\ninput = torch.randn(1, 16, dtype=torch.float16)\noutput = model(input)\n",
    "api": [
        "nn.Linear",
        "nn.Conv2d",
        "nn.MultiheadAttention",
        "nn.LSTM",
        "nn.GRU"
    ]
}