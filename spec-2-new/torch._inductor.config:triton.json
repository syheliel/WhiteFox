{
    "summary": "\nThe triton.codegen_upcast_to_fp32 configuration controls whether float16/bfloat16 operations are automatically upcast to float32 during Triton code generation. This setting is important because:\n1. Upcasting to fp32 can improve numerical stability for some operations\n2. However, it may lead to unnecessary precision conversion overhead\n3. Some models may work perfectly fine with lower precision\n4. Automatic upcasting could mask precision-related bugs that would appear in production\n```\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10, 10)\n        \n    def forward(self, x):\n        return self.linear(x.half())  # Use fp16 input\n\nmodel = SimpleModel().cuda()\ninput = torch.randn(1, 10).cuda().half()\noutput = model(input)  # Will be affected by codegen_upcast_to_fp32 setting\n```\n\n```yaml\n- nn.Linear\n- nn.functional.linear\n- torch.half\n- torch.bfloat16\n- torch.float32\n",
    "python_code": "\nimport torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10, 10)\n        \n    def forward(self, x):\n        return self.linear(x.half())  # Use fp16 input\n\nmodel = SimpleModel().cuda()\ninput = torch.randn(1, 10).cuda().half()\noutput = model(input)  # Will be affected by codegen_upcast_to_fp32 setting\n```\n\n```yaml\n- nn.Linear\n- nn.functional.linear\n- torch.half\n- torch.bfloat16\n- torch.float32\n",
    "api": [
        "nn.Linear",
        "nn.functional.linear",
        "torch.half",
        "torch.bfloat16",
        "torch.float32"
    ]
}