{
    "summary": "\nThe skip_nnmodule_hook_guards configuration controls whether PyTorch Dynamo will guard against changes to hooks on nn.Module instances. When set to True:\n1. Dynamo will not detect if hooks are added/removed/modified after compilation\n2. This can lead to incorrect execution if hooks change between compilations\n3. The optimization assumes hooks remain constant after first compilation\n4. This is unsafe if the model actually uses hooks that may change\n",
    "python_code": "\nimport torch\nimport torch.nn as nn\n\nclass ModelWithHook(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(10, 10)\n        self.hook_handle = None\n\n    def forward(self, x):\n        return self.layer(x)\n\nmodel = ModelWithHook()\ndef hook(module, inp, out):\n    return out * 2\n\n# First compilation with hook\nmodel.hook_handle = model.layer.register_forward_hook(hook)\noptimized_model = torch.compile(model)\nx = torch.randn(10)\nout1 = optimized_model(x)  # Will apply hook\n\n# Remove hook after compilation\nmodel.hook_handle.remove()\nout2 = optimized_model(x)  # With skip_nnmodule_hook_guards=True, may still apply hook incorrectly\n",
    "api": [
        "nn.Module.register_forward_hook",
        "nn.Module.register_backward_hook",
        "nn.Module.register_full_backward_hook",
        "nn.Module.register_module_forward_hook"
    ]
}