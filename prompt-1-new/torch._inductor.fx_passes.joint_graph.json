
{
	"remove_no_ops": {
		"hints": [
			{
				"type": "precision",
				"target_line": "if not fake_tensors_eq(node.meta[\"val\"], replacement.meta[\"val\"]):",
				"func": "remove_no_ops",
				"comment": "Precision loss possible when comparing tensor metadata"
			},
			{
				"type": "argument_check",
				"target_line": "if not all(isinstance(arg, torch.fx.Node) for arg in node.args):",
				"func": "remove_no_ops",
				"comment": "Incomplete argument type checking"
			}
		]
	},
	"UniformValueConstantFolder": {
		"hints": [
			{
				"type": "precision",
				"target_line": "self.node_replacements[node] = tensor.flatten()[0].item()",
				"func": "UniformValueConstantFolder",
				"comment": "Potential precision loss when converting tensor to scalar"
			},
			{
				"type": "quantization",
				"target_line": "if fake_tensor.dtype in (torch.uint8, torch.uint16, torch.uint32, torch.uint64):",
				"func": "UniformValueConstantFolder",
				"comment": "Special handling of uint types may affect quantization"
			}
		]
	},
	"canonicalize_quant_mapping": {
		"hints": [
			{
				"type": "quantization",
				"target_line": "invoke_quant_replacement.meta[\"quant_options\"] = quant_options",
				"func": "canonicalize_quant_mapping",
				"comment": "Quantization options handling may be incomplete"
			}
		]
	},
	"mul_softmax_pattern": {
		"hints": [
			{
				"type": "precision",
				"target_line": "return (inp - max_) * (sign * other)",
				"func": "mul_softmax_pattern",
				"comment": "Numerical stability issues possible in softmax pattern"
			}
		]
	},
	"div_softmax_pattern": {
		"hints": [
			{
				"type": "precision",
				"target_line": "return (inp - max_) / (sign * other)",
				"func": "div_softmax_pattern",
				"comment": "Numerical stability issues possible in softmax pattern"
			}
		]
	}
}
